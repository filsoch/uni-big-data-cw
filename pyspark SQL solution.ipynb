{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eedae18-09de-453c-b04e-74325a27d1d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87ab411-32ea-41e4-8565-c8d409326c69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/large.json.gz\"\n",
    "file_type = \"json\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_journal_info = \"/FileStore/tables/journal_information.csv\"\n",
    "journal_info = spark.read.option('header', True).csv(path_journal_info)\n",
    "\n",
    "#pre-processing of the journal file\n",
    "journal_info = journal_info.withColumnRenamed(\"Journal Name\", \"journal_name\") \n",
    "journal_info = journal_info.withColumnRenamed(\"Category & Journal Quartiles\", \"category_and_journal_quartiles\") \n",
    "\n",
    "#removing null row \n",
    "journal_info = journal_info.dropna(subset=['journal_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d449026d-0cbc-4d41-87be-0f033b66bce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a view for analysis of Q1, Q2, Q3\n",
    "\n",
    "temp_table_name = (\"publications\")\n",
    "df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2551e0-9105-426e-a755-b146e8c2b4ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 150000\nNumber of distinct IDs: 150000\nAll papers have unique IDs.\n"
     ]
    }
   ],
   "source": [
    "#question 1\n",
    "\n",
    "temp_table_name = (\"publications\")\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "\n",
    "#using pyspark sql to query the data\n",
    "distinct_papers_no = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT corpusid) AS distinct_id_count,\n",
    "        COUNT(*) AS total_count\n",
    "    FROM publications\n",
    "\"\"\")\n",
    "\n",
    "#converting the output of pyspark sql query (dataframe) into a list variable\n",
    "result_data = distinct_papers_no.collect()[0]\n",
    "distinct_id_count = result_data['distinct_id_count']\n",
    "total_count = result_data['total_count']\n",
    "\n",
    "# Outputting the results\n",
    "print(f\"Total number of papers: {total_count}\")\n",
    "print(f\"Number of distinct IDs: {distinct_id_count}\")\n",
    "\n",
    "# Checking if all papers have unique IDs\n",
    "if distinct_id_count == total_count:\n",
    "    print(\"All papers have unique IDs.\")\n",
    "else:\n",
    "    print(\"There are duplicate IDs in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec730ebf-c15b-4e2f-881a-d01971cd7f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of authors per each paper: 2.81628\n"
     ]
    }
   ],
   "source": [
    "#question 2\n",
    "\n",
    "temp_table_name = (\"publications\")\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "\n",
    "avg_authors = spark.sql(\"\"\"\n",
    "    SELECT AVG(size(authors)) AS average_num_authors\n",
    "    FROM publications\n",
    "\"\"\")\n",
    "\n",
    "avg_authors_result = avg_authors.collect()[0][0]\n",
    "print(f\"Average number of authors per each paper: {avg_authors_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccabdccb-76b6-49dc-9f7f-f1fba41f92be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique journals: 33916\n"
     ]
    }
   ],
   "source": [
    "#question 3\n",
    "\n",
    "temp_table_name = (\"publications\")\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "\n",
    "journal_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT journal.name) AS different_journals_count\n",
    "    FROM publications\n",
    "    WHERE journal.name IS NOT NULL AND TRIM(journal.name) != ''\n",
    "\"\"\")\n",
    "\n",
    "journal_count_result = journal_count.collect()[0][0]\n",
    "\n",
    "print(f\"Total number of unique journals: {journal_count_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414739f0-83d3-42c4-ab52-e00c2b0edaa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author</th><th>publication_count</th></tr></thead><tbody><tr><td>List(2149377746, B. Noble)</td><td>23</td></tr><tr><td>List(90537224, S. Sukhoruchkin)</td><td>16</td></tr><tr><td>List(88842366, Z. Soroko)</td><td>16</td></tr><tr><td>List(49898687, M. Kumar)</td><td>15</td></tr><tr><td>List(49611617, M. Jain)</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "2149377746",
          "B. Noble"
         ],
         23
        ],
        [
         [
          "90537224",
          "S. Sukhoruchkin"
         ],
         16
        ],
        [
         [
          "88842366",
          "Z. Soroko"
         ],
         16
        ],
        [
         [
          "49898687",
          "M. Kumar"
         ],
         15
        ],
        [
         [
          "49611617",
          "M. Jain"
         ],
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"authorId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "publication_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#question 4\n",
    "\n",
    "exploded_df = df.selectExpr(\"corpusid\", \"explode(authors) as author\")\n",
    "exploded_df.createOrReplaceTempView(\"exploded_publications\")\n",
    "\n",
    "\n",
    "top_authors_result = spark.sql(\"\"\"\n",
    "    SELECT author, COUNT(*) AS publication_count\n",
    "    FROM exploded_publications\n",
    "    GROUP BY author\n",
    "    ORDER BY publication_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "top_authors_result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf8ae84-453f-439e-ba88-d036ea099fb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author_name</th><th>cumulative_impact_factor</th></tr></thead><tbody><tr><td>List(2155504929, Ying Li)</td><td>93.832</td></tr><tr><td>List(144797099, M. Viana)</td><td>92.238</td></tr><tr><td>List(5152451, L. Andrade)</td><td>92.238</td></tr><tr><td>List(49900836, H. Wood)</td><td>90.422</td></tr><tr><td>List(7695437, A. M. Ruscio)</td><td>87.899</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "2155504929",
          "Ying Li"
         ],
         93.832
        ],
        [
         [
          "144797099",
          "M. Viana"
         ],
         92.238
        ],
        [
         [
          "5152451",
          "L. Andrade"
         ],
         92.238
        ],
        [
         [
          "49900836",
          "H. Wood"
         ],
         90.422
        ],
        [
         [
          "7695437",
          "A. M. Ruscio"
         ],
         87.899
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author_name",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"authorId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "cumulative_impact_factor",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#question 5\n",
    "\n",
    "selected_columns = df.select(col(\"journal.name\").alias(\"journal_name\"), explode(\"authors\").alias(\"authors\"))\n",
    "\n",
    "# Creating temporary views\n",
    "selected_columns.createOrReplaceTempView(\"papers\")\n",
    "journal_info.createOrReplaceTempView(\"journals\")\n",
    "\n",
    "top_authors_if_result = spark.sql(\"\"\"\n",
    "    SELECT authors AS author_name, SUM(IF) AS cumulative_impact_factor\n",
    "    FROM papers\n",
    "    LEFT JOIN journals ON papers.journal_name = journals.journal_name\n",
    "    GROUP BY authors\n",
    "    ORDER BY cumulative_impact_factor DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# Showing the results table\n",
    "top_authors_if_result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831e3bd2-8425-42fd-8f5a-9856c066c221",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>year</th><th>publications_count</th></tr></thead><tbody><tr><td>2020</td><td>444</td></tr><tr><td>2019</td><td>396</td></tr><tr><td>2018</td><td>365</td></tr><tr><td>2017</td><td>329</td></tr><tr><td>2016</td><td>283</td></tr><tr><td>2015</td><td>244</td></tr><tr><td>2014</td><td>242</td></tr><tr><td>2013</td><td>178</td></tr><tr><td>2012</td><td>165</td></tr><tr><td>2011</td><td>139</td></tr><tr><td>2010</td><td>112</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2020,
         444
        ],
        [
         2019,
         396
        ],
        [
         2018,
         365
        ],
        [
         2017,
         329
        ],
        [
         2016,
         283
        ],
        [
         2015,
         244
        ],
        [
         2014,
         242
        ],
        [
         2013,
         178
        ],
        [
         2012,
         165
        ],
        [
         2011,
         139
        ],
        [
         2010,
         112
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "publications_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# question 6\n",
    "\n",
    "selected_columns_for_q6 = df.select(col(\"journal.name\").alias(\"journal_name\"), \"year\")\n",
    "selected_columns_for_q6.createOrReplaceTempView(\"papers_trends\")\n",
    "journal_info.createOrReplaceTempView(\"journals\")\n",
    "\n",
    "publications_count_result = spark.sql(\"\"\"\n",
    "    SELECT year, COUNT(*) AS publications_count\n",
    "    FROM papers_trends\n",
    "    LEFT JOIN journals ON papers_trends.journal_name = journals.journal_name\n",
    "    WHERE IF >= 1 AND year BETWEEN 2010 AND 2020\n",
    "    GROUP BY year\n",
    "    ORDER BY year DESC\n",
    "\"\"\")\n",
    "\n",
    "publications_count_result.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark SQL solution",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
